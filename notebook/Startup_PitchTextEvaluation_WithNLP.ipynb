{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XdmbY0jiZQPg"
   },
   "source": [
    "# **Startup Pitch Text Evaluation with NLP**\n",
    "\n",
    "**Hello, I’m Mohan Raj Murugesan, a student at Coimbatore Institute of Technology, contactable at [Mohan Raj Murugesan](https://www.linkedin.com/in/mohan-raj-m-450560225). My project, *Startup Pitch Text Evaluation with NLP*, is an pipeline designed to evaluate startup pitch decks for investors, developed as part of the `Startup_PitchTextEvaluation_WithNLP` notebook.**\n",
    "\n",
    "---\n",
    "\n",
    "The purposed pipeline processes PDF pitch decks, extracting text, categorizing it into investor-relevant sections (e.g., Problem, Market), and scoring them using advanced NLP techniques. It computes a normalized final score (0–100) by combining BART’s zero-shot quality scores (70% weight) and VADER’s sentiment scores (30% weight) across six dimensions—Problem, Market, Traction, Team, Business Model, Vision/Moat, plus a deck-wide Confidence score. The system also generates summaries and insights with T5, classifies industries using BART with a keyword-based fallback, clusters decks with KMeans, and presents results through an interactive Dash dashboard featuring radar charts, bar charts, heatmaps, and word clouds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RcbLrG1dW7Tn"
   },
   "source": [
    "# Downloading Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tPdoFiXjV4Ha",
    "outputId": "422cd333-2e0a-4240-b842-fb076bb8d7b7"
   },
   "outputs": [],
   "source": [
    "%pip install pdfplumber\n",
    "%pip install vaderSentiment\n",
    "%pip install dash\n",
    "!pip install pdfplumber spacy vaderSentiment transformers pandas plotly==5.15.0 kaleido==0.2.1 pytesseract\n",
    "!apt-get install -y tesseract-ocr libtesseract-dev\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mH-Y0vsPXCek"
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y0Hr3Va3i6JW"
   },
   "source": [
    "\n",
    "The README provides a clear summary:\n",
    "- **Inputs**: Pitch deck PDFs in a folder (`/content/pdf_decks`).\n",
    "- **Processes**: Text extraction, categorization, sentiment analysis, scoring, summarization, and clustering.\n",
    "- **Outputs**: A DataFrame with analysis results, visualizations (radar, bar, heatmap, word cloud), and an interactive dashboard.\n",
    "- **Dependencies**: Libraries for PDF processing (`pdfplumber`, `pytesseract`), NLP (`spacy`, `vaderSentiment`, `transformers`), visualization (`plotly`, `dash`, `wordcloud`), and clustering (`scikit-learn`).\n",
    "\n",
    "Key Components and Methods\n",
    "  - **pdfplumber, pytesseract, PIL**: Handle PDF text extraction and OCR for image-based content.\n",
    "  - **spacy, vaderSentiment, transformers**: Enable NLP tasks (entity recognition, sentiment analysis, scoring, summarization).\n",
    "  - **pandas, numpy**: Manage and process data (e.g., storing scores, summaries, and metadata in a DataFrame).\n",
    "  - **plotly, dash, matplotlib, wordcloud**: Create visualizations (radar charts, bar charts, heatmaps, word clouds) and an interactive dashboard.\n",
    "  - **os, pathlib, re**: Handle file operations and text preprocessing.\n",
    "  - **sklearn.cluster.KMeans**: Groups pitch decks into clusters for comparative analysis.\n",
    "  - **logging**: Tracks pipeline progress and errors.\n",
    "  - **google.colab.files**: Supports file downloads in Colab environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dKSHEsi3WPjh"
   },
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import dash\n",
    "from dash import dcc, html\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import io\n",
    "import base64\n",
    "from sklearn.cluster import KMeans\n",
    "import logging\n",
    "try:\n",
    "    from google.colab import files  # For Colab downloads\n",
    "except ImportError:\n",
    "    files = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nUoRsWUuXFIS"
   },
   "source": [
    "# Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "id": "_7pN5jNxW3A8",
    "outputId": "4882c6d7-2eb2-4f2c-e5de-1807c4383027"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "!mkdir -p /content/pdf_decks\n",
    "!mv *.pdf /content/pdf_decks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shfkePgBXKbP"
   },
   "source": [
    "# Initialize NLP tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWj-jqq_i6JW"
   },
   "source": [
    "### Why These Methods?\n",
    "- **spaCy (`en_core_web_sm`)**: Chosen for its lightweight efficiency and robust entity recognition, ideal for parsing structured pitch deck sections.\n",
    "- **VADER**: Selected for its speed and suitability for short, persuasive texts, providing a quick sentiment metric relevant to investor perception.\n",
    "- **BART (`facebook/bart-large-mnli`)**: Used for its zero-shot flexibility, enabling scoring across diverse dimensions without training data, a key advantage for a generalizable pipeline.\n",
    "- **T5 (`t5-base`)**: Preferred for its abstractive summarization capabilities, delivering concise, investor-friendly summaries.\n",
    "- **BART Tokenizer**: Essential for preprocessing text to match BART’s requirements, ensuring accurate scoring.\n",
    "- **Zero-Shot Classification (BART)**: Using BART for scoring pitch decks is experimental because zero-shot classification is less common in this domain than supervised methods. It’s innovative as it eliminates the need for a labeled dataset of scored decks, which is often unavailable. However, its accuracy depends on well-crafted prompts, and it may require validation against human investor scores.\n",
    "- **T5 Summarization**: While T5 is a standard choice for summarization, applying it to pitch decks (which have varied structures and jargon) is somewhat experimental. The model’s ability to generate abstractive summaries ensures flexibility but may need fine-tuning for domain-specific terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 533,
     "referenced_widgets": [
      "e9a70c7ebff047eb93bdc592fca52572",
      "730373e6a77c4cdb926533b32bd39019",
      "8f6fa7f2b108451faefa03be1242e0b8",
      "76d0f12e66fd4d5a9d7e2d114b9a2472",
      "af78ef4ee47f48da959d4b16940416aa",
      "53d6078dcb324d3f9d9663bf405f5302",
      "652ca22e1e484d9ba3e1da058061081d",
      "cfe88b34364e43e491ea8f44ff0be21d",
      "a4cc35e071864b1c9b1f4e6c95de2012",
      "1e54eb17565c4b2da2f5cdc9c1c264e7",
      "ef4418b391df4b4381792519ca7fc088",
      "2039ea3248f74505afa13fca10e670a4",
      "3ecbb79138aa42198f5d40bc75dda37c",
      "59079bf3672a4f569019b0ea96207e90",
      "8a6e55293aa444109f818b2ea192a20e",
      "56e2b63940eb4257941d95bfecd068ef",
      "2593432b9ad94ab89227728c80540183",
      "c005d7a1be3840fa9d6f20aa6086ae2d",
      "ccadba7c5d984c11b5cacd7cc9c1c296",
      "5214dd6337a74d3280be681a81861461",
      "a672fbd171884f6480e8b3e14ff3a262",
      "5639e629966a4149b48f04569f932f90",
      "3bfd8634ae074addadba86cdf8ff295a",
      "a7563abf4a6544fc928c0e6ea420a33b",
      "c16d32afba58416bbb8e683cdc675424",
      "761b8028e7934f4cb71ef6e6b44a23b5",
      "845cf419524845b6b87669da2a4aa9b1",
      "2e2dce7ae9264592bfc35147831aeb99",
      "2ba39e782bf341f59ad8d958e9d2fdd9",
      "92b98227119c4a79b6ad276b2e0950b0",
      "e4cad5be85fb42a5bfe0dfdc85870fe6",
      "cf8c639f43664369ad5655f437136fef",
      "bc93bb8d48bb47e48e9d5c057cef2fd0",
      "8bb008ad15044305b3abdaf0d72f9ea6",
      "a2e2ea41afec46669798d38cc2373b81",
      "641ab76fd6bb4ff297aab827063eb449",
      "04532a5d3d814dfa8a27f1a7c0dbcafb",
      "8a7db53fe67c44d0a3360f05ee3f3c7a",
      "42e0d3cf244d4038800542a93b20fd72",
      "3d656712489f4d51a5425506b5fb2712",
      "678423b4219c43dcb0faaae986d00a94",
      "0596d0c9e6d14d9c812b298ea218ae1d",
      "3cca9d85823f48a599cad9662d25cf8e",
      "af029075eb7f48d09ad69d71aefc018f",
      "ac69581ffcad4ad79d682415948325ad",
      "97d0aa6f46b54e2ca72758e320d1deeb",
      "939dbc002d16458a92fd7be79c1e975e",
      "51508c207b8f4393bc0480eb93c1e984",
      "8789e5d485204a2c9aa41b0ffffb985e",
      "141bae12237e46c0b13b0a657b58cec9",
      "ce9659dc58b74b229134ac89440b2b2c",
      "1c407f13f6f04610aa09d96d782de2a4",
      "b1f3550cbb084b4398d8070e21a1d80d",
      "1b184bd363214d208bed4a7816bb5cdb",
      "7bfe16e71df14b34811d8b646f2f1f46",
      "c294d909f3f84e1c97c3dd6f5b1883b2",
      "37d43ab848a644efb52227897e670d66",
      "1b8be0e42dda47c3b7954c62e33cf6a4",
      "595b02e30f2246c8934323f77bc7558d",
      "3de0d63d3a00432db29e75e83baa993b",
      "d66e23f5e1aa4c6e8a60550af2b025a5",
      "cd9ed6ca1b60437b8ccd692eb85ae280",
      "067bc9294e8b48c2a7ac7180f819c8ce",
      "2ce332d2b98f44da8c9416ef0cf401b9",
      "a7761a2000264c51a20c38f0956e195c",
      "027e14d45db24003aaa09a4fc3087532",
      "1c1ee19e03c44a3fab02975dc0f55572",
      "ba40c1a5bf77450daf2b9b4c3b72c683",
      "24b41896bda148d8a4e87b92d0564dc1",
      "2ab72d8e62ed43f496c3597dbd24510a",
      "9fcc623e75d74eef83fb2fbaa48d9e60",
      "b70b495d8643488c9defe10cd31bc4fd",
      "43b12dceacf14244ae52f716ac8cd822",
      "0388e54265e74ef099d5f35b10734a07",
      "e8836465142d49fc93d1e3a0fa107420",
      "7667770067d543ff92d73e76b846cc0a",
      "494f3090df1f4c208de60d6ceb500a88",
      "19670f270b27438895f879ffef63c356",
      "171cd2cbeddf45f8941a0c541db2e35a",
      "18c290838d8542d2bf3fdf413fbb3f3c",
      "bd1e9f174a834e6c82b8d63156399bf4",
      "8f3e81833fe84d88bfec0541c68378ce",
      "ca1f75633e4c4e84b5a298792ebe3f71",
      "52dfb76472494b2e9d48ece3ed496155",
      "ecd6ec4d34d140a7ac593c486807b40c",
      "c90b3d130c94441d9c8329d1ad67e76f",
      "704703df32a94f3aaedcde5d37b7106d",
      "e85b436c4c2445989378cca5dd51f18a",
      "6d43c5984fcb48ed926bf4dfbc5c47cc",
      "67a33e8b2cd349db91fc0229846b0cda",
      "74f5b48f9e6b47f4b715b6fb1818e3f1",
      "19352000ba184fa6a8a0104b12c9988f",
      "110906520fd44ac6b78046b1a1da2d3d",
      "d621f57478154cd5830c087971dd7fb6",
      "529b1e6d52d24fc8ad434f2470c746eb",
      "6975880e5f034305b2d697bf1cfd29ff",
      "8bf49436ed39416991f74ad2c44cbdec",
      "2b3af629876a48e199d7a6a458352ac2",
      "6b5e219368304017bc37b6b612c36c55",
      "c32022d5406f4aa3b35ab7741517ec62",
      "94dc52e3485b4d2abc5d5678157b1413",
      "002ea6657efa448aa5f988dd48aa0ae6",
      "ef9576d0836b4529bd94e29b731beacd",
      "18e34d80ca6d43e18fee5f89747b719f",
      "470e82a1c73d4259ab61df5655aa05c7",
      "5a86148384984baf9e60c1ac2d3e7e03",
      "6fc91508fdf34cefa0feaf35f8f48f2a",
      "cc34e84ec59d4e4d9e808b6446496331",
      "a8524b5c88cb4b42952e2c3f3c3dc52d",
      "cc317879fc9d40548a28e710fef7cc48",
      "caf53668172f4aaa9dab720d675d94c2",
      "47c502b7644e4b5190b32335fb5c0695",
      "e49457b623a14ee7bc709407cef53ed3",
      "74db423ee2234a86ad9bb958822c30e9",
      "9f3a4bd6288e48c182aa5ad1d5db648e",
      "869e0c3bb6aa4f4297f38206d21241d7",
      "43a01d47b4ab48a5942759f3371c7ea2",
      "488c54bbd3634488b8fbe1d437a9a2b5",
      "e49644e33f9c46b4b5c5fd1ac9175fd7",
      "7c6ccf6f9cc647a883e39d73449182a9",
      "a4f0ab7fe2ba495da7b2509be14af5f0"
     ]
    },
    "id": "rU8a8dZHWcqC",
    "outputId": "f086cc19-993a-447f-cf1d-75cc36cbecf3"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "zero_shot = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "summarizer = pipeline(\"summarization\", model=\"t5-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6s2Y61aJrImH"
   },
   "source": [
    "# Placeholder for extract_text_hybrid_easyocr (assumed to return list of slide texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1i8mdDei6JX"
   },
   "source": [
    "- **Text Extraction (`extract_text_hybrid_easyocr`)**:\n",
    "  - This function is the first step in the pipeline, converting raw PDFs into usable text. It feeds into subsequent steps like section parsing (via spaCy), sentiment analysis (VADER), scoring (BART), and summarization (T5).\n",
    "  - It combines `pdfplumber` for text-based PDFs and `pytesseract` for image-based content, ensuring comprehensive text extraction. Grayscale conversion and specific Tesseract configurations optimize OCR for pitch deck layouts.\n",
    "  - **Logic**: Ensures all content is captured, whether text-based or image-based, making the pipeline robust to diverse PDF formats.\n",
    "- **Text Cleaning (`clean_text`)**:\n",
    "  - Prepares extracted text for NLP tasks by removing noise, ensuring models focus on meaningful content.\n",
    "  - Removes formatting noise (newlines, page numbers, fractions) to prepare text for NLP tasks, focusing on meaningful content.\n",
    "  - **Logic**: Cleansed text improves the accuracy of downstream tasks like summarization and scoring by reducing irrelevant tokens.\n",
    "\n",
    "The hybrid extraction approach maximizes coverage across PDF types, while the cleaning function ensures high-quality input for summarization and scoring. The methods are practical, leveraging lightweight tools (`pdfplumber`, `pytesseract`, regex) suitable for pitch deck analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rs2bw-HCrE2V"
   },
   "outputs": [],
   "source": [
    "def extract_text_hybrid_easyocr(pdf_file):\n",
    "    \"\"\"Extract text from PDF using hybrid method (pdfplumber + EasyOCR).\"\"\"\n",
    "    text = []\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_file) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text and len(page_text.strip()) > 10:\n",
    "                    text.append(page_text.strip())\n",
    "                else:\n",
    "                    # Fallback to OCR (placeholder for EasyOCR)\n",
    "                    try:\n",
    "                        img = page.to_image(resolution=300).original\n",
    "                        img = img.convert(\"L\")  # Grayscale\n",
    "                        page_text = pytesseract.image_to_string(img, config='--psm 6 --oem 3')\n",
    "                        text.append(page_text.strip())\n",
    "                    except Exception as e:\n",
    "                        logging.warning(f\"OCR failed for {pdf_file} on page {page.page_number}: {e}\")\n",
    "                        text.append(\"\")\n",
    "        return text if text else [\"No text extracted\"]\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {pdf_file}: {e}\")\n",
    "        return [\"No text extracted\"]\n",
    "\n",
    "# Placeholder for clean_text (assumed to clean text for summarization)\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text for summarization.\"\"\"\n",
    "    text = re.sub(r\"\\n\\s*\\n\", \"\\n\", text)\n",
    "    text = re.sub(r\"\\d+/\\d+\", \"\", text)\n",
    "    text = re.sub(r\"(\\bpage\\b|\\b\\d+\\b)\", \"\", text, flags=re.IGNORECASE)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2POl_6LpFCD"
   },
   "source": [
    "# Helper function to truncate text to max tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wUJmgyC3qqTr"
   },
   "outputs": [],
   "source": [
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ig7TRPObi6JX"
   },
   "source": [
    "The `truncate_to_max_tokens` function tokenizes input text using the BART tokenizer, truncates it to fit within a 512-token limit, and decodes it back to a string, ensuring compatibility with transformer models like BART and T5. It’s a critical preprocessing step to prevent model errors due to excessive input length.\n",
    "- **The function uses the model-specific BART tokenizer to ensure accurate tokenization for scoring, with a default 512-token limit suitable for pitch deck content. Decoding ensures the output is usable for downstream tasks. The approach is simple and robust, aligning with the pipeline’s need to process concise, structured text.**\n",
    "- **Using the BART tokenizer for both BART and T5 tasks is a pragmatic simplification but could be refined by adding a T5-specific tokenizer for better summarization performance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I-0RLZGrpDlI"
   },
   "outputs": [],
   "source": [
    "def truncate_to_max_tokens(text, max_tokens=512):\n",
    "    \"\"\"Truncate text to fit within max_tokens for the model.\"\"\"\n",
    "    tokens = tokenizer(text, truncation=True, max_length=max_tokens, return_tensors=\"pt\")\n",
    "    truncated_text = tokenizer.decode(tokens[\"input_ids\"][0], skip_special_tokens=True)\n",
    "    return truncated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jkhvWJ4zXMlu"
   },
   "source": [
    "# Step 1: Categorize Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9F8OzTCi6JY"
   },
   "source": [
    "### Purpose\n",
    "The `categorize_text` function processes extracted text from pitch deck pages (e.g., from `extract_text_hybrid_easyocr`) and organizes it into predefined sections (Problem, Solution, Market, etc.) using a combination of keyword matching and spaCy’s named entity recognition (NER). This structured output facilitates downstream tasks like scoring (BART) and summarization (T5).\n",
    "\n",
    "---\n",
    "\n",
    "The function `extract_text_hybrid_easyocr` follows text extraction and cleaning `clean_text`, and precedes scoring (BART) and summarization (T5). It structures raw text into investor-relevant sections for targeted analysis.\n",
    "- **By organizing text into sections like \"Problem\" or \"Market\", the function enables:**\n",
    "  - **Scoring**: BART can score specific sections (e.g., \"Problem Clarity\" based on the \"Problem\" text).\n",
    "  - **Summarization**: T5 can summarize individual sections or the entire deck using categorized text.\n",
    "  - **Analysis**: Sections like \"Traction\" or \"Business Model\" provide focused data for investor insights.\n",
    "- **Example Workflow**:\n",
    "  1. Extract text from a PDF (`extract_text_hybrid_easyocr`).\n",
    "  2. Clean the text (`clean_text`).\n",
    "  3. Categorize the text into sections (`categorize_text`).\n",
    "  4. Truncate section text (`truncate_to_max_tokens`) and feed to BART for scoring or T5 for summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RPD5nZotWg2J"
   },
   "outputs": [],
   "source": [
    "def categorize_text(pages):\n",
    "    \"\"\"Categorize text into sections using keywords and spaCy entities.\"\"\"\n",
    "    sections = {\n",
    "        \"Problem\": [\"problem\", \"challenge\", \"pain point\", \"issue\", \"need\", \"obstacle\", \"difficulty\", \"barrier\"],\n",
    "        \"Solution\": [\"solution\", \"product\", \"offering\", \"solve\", \"platform\", \"service\", \"technology\", \"app\"],\n",
    "        \"Market\": [\"market\", \"TAM\", \"SAM\", \"SOM\", \"opportunity\", \"industry\", \"segment\", \"potential\", \"demand\"],\n",
    "        \"Traction\": [\"traction\", \"growth\", \"users\", \"revenue\", \"metrics\", \"customers\", \"sales\", \"adoption\", \"engagement\"],\n",
    "        \"Team\": [\"team\", \"founder\", \"experience\", \"background\", \"advisor\", \"staff\", \"leadership\", \"executive\", \"member\"],\n",
    "        \"Business Model\": [\"business model\", \"revenue\", \"monetization\", \"pricing\", \"subscription\", \"income\", \"profit\", \"model\"],\n",
    "        \"Vision/Moat\": [\"vision\", \"moat\", \"advantage\", \"IP\", \"patent\", \"defensibility\", \"strategy\", \"differentiation\", \"unique\"]\n",
    "    }\n",
    "    categorized = {key: [] for key in sections}\n",
    "    for page in pages:\n",
    "        if page == \"No text extracted\":\n",
    "            continue\n",
    "        doc = nlp(page)\n",
    "        current_section = None\n",
    "        for line in page.split(\"\\n\"):\n",
    "            line_lower = line.lower()\n",
    "            for section, keywords in sections.items():\n",
    "                if any(keyword in line_lower for keyword in keywords):\n",
    "                    current_section = section\n",
    "                    break\n",
    "            if current_section:\n",
    "                categorized[current_section].append(line)\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in [\"PERSON\", \"ORG\"] and any(keyword in page.lower() for keyword in sections[\"Team\"]):\n",
    "                categorized[\"Team\"].append(ent.text)\n",
    "            elif ent.label_ in [\"MONEY\", \"CARDINAL\"] and any(keyword in page.lower() for keyword in sections[\"Market\"]):\n",
    "                categorized[\"Market\"].append(ent.text)\n",
    "    return {key: \" \".join(set(val)) for key, val in categorized.items() if val}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HO2E4AEXRxY"
   },
   "source": [
    "# Step 2: Score Each Deck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXf1aidUi6JY"
   },
   "source": [
    "### Overview\n",
    "The `score_dimension` function scores a single section of a pitch deck (e.g., Problem, Market) based on its text, using VADER for sentiment analysis and BART for zero-shot classification. The `score_deck` function aggregates scores across multiple dimensions and computes a normalized final score, incorporating a Confidence score for the entire deck. These functions rely on text categorized by `categorize_text`, cleaned by `clean_text`, and extracted by `extract_text_hybrid_easyocr`.\n",
    "\n",
    "---\n",
    "### How Scoring Is Done\n",
    "1. **Per-Dimension Scoring (`score_dimension`)**:\n",
    "   - **Input**: Text for a section (e.g., Problem text from `categorize_text`).\n",
    "   - **Process**:\n",
    "     - **Sentiment**: VADER computes a `compound` score (-1 to +1) for the first 500 characters, reflecting tone.\n",
    "     - **Quality**: BART evaluates the first 500 characters against criteria (e.g., \"clear and specific\"), producing a probability (0–1) scaled to 0–10.\n",
    "     - **Combined Score**:\n",
    "       \\[\n",
    "       dimension_score = 0.7 x (BART score x 10) + 0.3 x(VADER compound + 1) x 5\n",
    "       \\]\n",
    "     - Weights prioritize quality (70%) over sentiment (30%).\n",
    "   - **Output**: A score (0–10) for the dimension.\n",
    "   - **Example**:\n",
    "     - Text: \"Our platform solves inefficiencies in healthcare delivery.\"\n",
    "     - VADER: `compound = 0.4` → `(0.4 + 1) * 5 = 7.0`.\n",
    "     - BART: `scores[\"clear and specific\"] = 0.8` → `0.8 * 10 = 8.0`.\n",
    "     - Score: \\( 0.7 x 8.0 + 0.3 x 7.0 = 5.6 + 2.1 = 7.7 \\).\n",
    "\n",
    "2. **Deck Scoring (`score_deck`)**:\n",
    "   - **Input**: Dictionary of section texts from `categorize_text`.\n",
    "   - **Process**:\n",
    "     - Scores six dimensions (Problem, Market, Traction, Team, Business Model, Vision/Moat) using `score_dimension`.\n",
    "     - Computes a Confidence score for the entire deck’s text:\n",
    "       \\[\n",
    "       text{Confidence} = (VADER compound x 5) + 5\n",
    "       \\]\n",
    "     - Sums all seven scores (6 dimensions + Confidence) to get `total` (0–70).\n",
    "     - Normalizes to 0–100:\n",
    "       \\[\n",
    "       normalized = (total/70) x 100\n",
    "       \\]\n",
    "   - **Output**: A dictionary of scores and a normalized final score.\n",
    "   - **Example**:\n",
    "     - Scores: Problem = 7.7, Market = 8.0, Traction = 6.5, Team = 7.0, Business Model = 6.8, Vision/Moat = 7.2, Confidence = 6.0.\n",
    "     - Total: \\( 7.7 + 8.0 + 6.5 + 7.0 + 6.8 + 7.2 + 6.0 = 49.2 \\).\n",
    "     - Normalized: \\( (49.2 / 70) x 100 = 70.3 \\).\n",
    "\n",
    "---\n",
    "\n",
    "### How the Final Score Is Calculated\n",
    "The final score (`normalized`) is a normalized percentage (0–100) based on the sum of seven scores (six dimensions + Confidence), each ranging from 0–10.\n",
    "\n",
    "1. **Dimension Scores**:\n",
    "   - Each of the six dimensions (Problem, Market, Traction, Team, Business Model, Vision/Moat) is scored using `score_dimension`:\n",
    "     \\[\n",
    "     dimension_score = 0.7 x (BART score x 10) + 0.3 x (VADER compound + 1) x 5\n",
    "     \\]\n",
    "   - Range: 0–10 per dimension.\n",
    "   - Total for six dimensions: 0–60.\n",
    "\n",
    "2. **Confidence Score**:\n",
    "   - Computed for the entire deck’s text:\n",
    "     \\[\n",
    "     Confidence = (VADER compound x 5) + 5\n",
    "     \\]\n",
    "   - Range: 0–10 (since `compound` is -1 to +1, scaled to 0–10).\n",
    "\n",
    "3. **Total Score**:\n",
    "   - Sum of all seven scores:\n",
    "     \\[\n",
    "     total = Problem + Market + Traction + Team + Business Model + Vision/Moat + Confidence\n",
    "     \\]\n",
    "   - Maximum: \\( 10 x 7 = 70 \\).\n",
    "\n",
    "4. **Normalized Final Score**:\n",
    "   - Scaled to 0–100:\n",
    "     \\[\n",
    "     normalized = (total/70) x 100\n",
    "     \\]\n",
    "   - Rounded to one decimal place for readability.\n",
    "\n",
    "5. **Example Calculation**:\n",
    "   - Assume scores: Problem = 7.7, Market = 8.0, Traction = 6.5, Team = 7.0, Business Model = 6.8, Vision/Moat = 7.2, Confidence = 6.0.\n",
    "   - Total: \\( 7.7 + 8.0 + 6.5 + 7.0 + 6.8 + 7.2 + 6.0 = 49.2 \\).\n",
    "   - Normalized: \\( (49.2 / 70) x = 70.3 \\).\n",
    "   - Output: `scores = {\"Problem\": 7.7, \"Market\": 8.0, ..., \"Confidence\": 6.0}, normalized = 70.3`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rc34-pSPWkmK"
   },
   "outputs": [],
   "source": [
    "def score_dimension(text, dimension, criteria):\n",
    "    \"\"\"Score a dimension using NLP.\"\"\"\n",
    "    if not text or text == \"No text extracted\":\n",
    "        return 0\n",
    "    sentiment = vader.polarity_scores(text[:500])[\"compound\"]\n",
    "    try:\n",
    "        scores = zero_shot(text[:500], candidate_labels=criteria, multi_label=False)\n",
    "        quality_score = scores[\"scores\"][0] * 10\n",
    "    except Exception as e:\n",
    "        print(f\"Zero-shot classification failed for {dimension}: {e}\")\n",
    "        quality_score = 0\n",
    "    return round(0.7 * quality_score + 0.3 * (sentiment + 1) * 5, 1)\n",
    "\n",
    "def score_deck(sections):\n",
    "    \"\"\"Score a deck across all dimensions.\"\"\"\n",
    "    criteria = {\n",
    "        \"Problem\": [\"clear and specific\", \"vague\", \"generic\"],\n",
    "        \"Market\": [\"large and quantified\", \"small\", \"unfocused\"],\n",
    "        \"Traction\": [\"strong metrics\", \"weak metrics\", \"no data\"],\n",
    "        \"Team\": [\"experienced and relevant\", \"inexperienced\", \"generic\"],\n",
    "        \"Business Model\": [\"clear monetization\", \"unclear\", \"unsustainable\"],\n",
    "        \"Vision/Moat\": [\"defensible and scalable\", \"generic\", \"weak\"]\n",
    "    }\n",
    "    scores = {}\n",
    "    for dim in criteria:\n",
    "        scores[dim] = score_dimension(sections.get(dim, \"\"), dim, criteria[dim])\n",
    "    overall_text = \" \".join(sections.values())\n",
    "    scores[\"Confidence\"] = round(vader.polarity_scores(overall_text)[\"compound\"] * 5 + 5, 1) if overall_text else 0\n",
    "    total = sum(scores.values())\n",
    "    normalized = round((total / 70) * 100, 1)\n",
    "    return scores, normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bYpes80KXT6V"
   },
   "source": [
    "# Step 3: Process All Decks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aemi_XlJi6JZ"
   },
   "source": [
    "### Overview\n",
    "The `process_decks` function is the core of the pitch deck analysis pipeline, orchestrating the following steps for each PDF in a specified folder:\n",
    "1. **Text Extraction**: Uses `extract_text_hybrid_easyocr` to extract text from PDFs.\n",
    "2. **Text Categorization**: Uses `categorize_text` to organize text into investor-relevant sections.\n",
    "3. **Scoring**: Uses `score_deck` to compute dimension scores and a normalized final score.\n",
    "4. **Summarization**: Uses `summarizer` (T5) to generate summaries and insights.\n",
    "5. **Industry Classification**: Uses `zero_shot` (BART) and keyword-based fallback to classify the deck’s industry.\n",
    "6. **Clustering**: Uses KMeans to group decks based on scores.\n",
    "7. **Output**: Returns a DataFrame with results, including scores, summaries, industry labels, suggestions, and cluster assignments.\n",
    "\n",
    "The final score, as computed by `score_deck`, is a normalized 0–100 score based on seven dimension scores (Problem, Market, Traction, Team, Business Model, Vision/Moat, Confidence).\n",
    "\n",
    "---\n",
    "#### Logic Breakdown\n",
    "1. **Setup**:\n",
    "   - Creates the `output_folder` if it doesn’t exist (`os.makedirs(output_folder, exist_ok=True)`).\n",
    "   - Defines a hardcoded list of `deck_names` (e.g., \"Pitch-Example-Air-BnB-PDF\", \"uber-pitch-deck\").\n",
    "   - Initializes an empty `results` list to store analysis for each deck.\n",
    "\n",
    "2. **Text Extraction**:\n",
    "   - Constructs the PDF file path (`pdf_file`) for each deck using `Path`.\n",
    "   - Checks if the PDF exists; if not, logs a warning and sets `text = [\"No text extracted\"]`.\n",
    "   - Otherwise, calls `extract_text_hybrid_easyocr` to extract text per page.\n",
    "   - Saves extracted text to a `.txt` file in `output_folder`, with each slide labeled (e.g., `---SLIDE 1---`).\n",
    "\n",
    "3. **Text Categorization**:\n",
    "   - Calls `categorize_text` to organize extracted text into sections (e.g., Problem, Market).\n",
    "\n",
    "4. **Scoring**:\n",
    "   - Calls `score_deck` to compute dimension scores (Problem, Market, Traction, Team, Business Model, Vision/Moat, Confidence) and a normalized final score (0–100).\n",
    "   - **Final Score Calculation** (from `score_deck`):\n",
    "     - Each dimension score (0–10) combines BART zero-shot quality (70%) and VADER sentiment (30%):\n",
    "       \\[\n",
    "       dimension_score = 0.7 x(BART score x 10) + 0.3 x (VADER compound + 1) x 5\n",
    "       \\]\n",
    "     - Confidence score (0–10):\n",
    "       \\[\n",
    "       Confidence = (VADER compound x 5) + 5\n",
    "       \\]\n",
    "     - Total score (0–70):\n",
    "       \\[\n",
    "       total = summation( dimension score + Confidence)\n",
    "       \\]\n",
    "     - Normalized final score (0–100):\n",
    "       \\[\n",
    "       final_score = (total/70) x 100\n",
    "       \\]\n",
    "   - Example:\n",
    "     - Scores: Problem = 7.7, Market = 8.0, Traction = 6.5, Team = 7.0, Business Model = 6.8, Vision/Moat = 7.2, Confidence = 6.0.\n",
    "     - Total: \\( 7.7 + 8.0 + 6.5 + 7.0 + 6.8 + 7.2 + 6.0 = 49.2 \\).\n",
    "     - Final Score: \\( (49.2 / 70) x 100 = 70.3 \\).\n",
    "\n",
    "5. **Summarization**:\n",
    "   - Concatenates all page texts, cleans the first 1000 characters (`clean_text`), and uses T5 (`summarizer`) to generate:\n",
    "     - A summary (20–50 tokens).\n",
    "     - An insight (5–10 tokens, based on the first 200 characters).\n",
    "   - Handles errors by logging and setting fallback messages.\n",
    "\n",
    "6. **Industry Classification**:\n",
    "   - Concatenates all text, truncates to 512 tokens (`truncate_to_max_tokens`), and uses BART (`zero_shot`) to classify the deck’s industry (e.g., Fintech, Social Media).\n",
    "   - **Fallback**: If the predicted label isn’t in the candidate list, uses a keyword-based approach:\n",
    "     - Counts weighted keywords (e.g., \"payment\" for Fintech) in the text.\n",
    "     - Boosts scores with spaCy NER (e.g., ORG, PRODUCT entities).\n",
    "     - Selects the industry with the highest score.\n",
    "   - Sets \"Unknown\" if classification fails.\n",
    "\n",
    "7. **Improvement Suggestions**:\n",
    "   - For each dimension with a score < 5, adds a suggestion to improve clarity or metrics.\n",
    "   - Joins suggestions with semicolons or sets a default message if none apply.\n",
    "\n",
    "8. **Results Storage**:\n",
    "   - Appends a dictionary to `results` with:\n",
    "     - Deck name, dimension scores, final score, summary, insight, industry, suggestions.\n",
    "\n",
    "9. **Clustering**:\n",
    "   - Converts `results` to a DataFrame (`df`).\n",
    "   - If multiple decks exist, applies KMeans clustering (up to 3 clusters) on dimension scores, filling missing values with 0.\n",
    "   - Adds a \"Cluster\" column to group similar decks.\n",
    "\n",
    "10. **Output**:\n",
    "    - Returns the DataFrame with all results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h78oFyh3Wnu4"
   },
   "outputs": [],
   "source": [
    "def process_decks(folder_path, output_folder):\n",
    "    \"\"\"Process all PDFs, extract text, and analyze.\"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    deck_names = [\n",
    "        \"Pitch-Example-Air-BnB-PDF\",\n",
    "        \"uber-pitch-deck\",\n",
    "        \"6737d05825e11f73f6d5a289_Ndc8GMUtaMNHOXDfqftyW1Jb7b5h2JE_ThY_Joc5Cf8\",\n",
    "        \"FACEBOOK\",\n",
    "        \"doordash-pitch-deck\"\n",
    "    ]\n",
    "    results = []\n",
    "\n",
    "    for deck_name in deck_names:\n",
    "        pdf_file = Path(folder_path) / f\"{deck_name}.pdf\"\n",
    "        txt_out_path = Path(output_folder) / f\"{deck_name}.txt\"\n",
    "\n",
    "        if not pdf_file.exists():\n",
    "            logging.warning(f\"File {pdf_file} not found\")\n",
    "            text = [\"No text extracted\"]\n",
    "        else:\n",
    "            text = extract_text_hybrid_easyocr(pdf_file)\n",
    "            with open(txt_out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                for i, slide in enumerate(text):\n",
    "                    f.write(f\"---SLIDE {i+1}---\\n{slide}\\n\\n\")\n",
    "\n",
    "        sections = categorize_text(text)\n",
    "        scores, final_score = score_deck(sections)\n",
    "\n",
    "        try:\n",
    "            clean_summary_text = clean_text(\" \".join(text)[:1000])\n",
    "            summary = summarizer(clean_summary_text, max_length=50, min_length=20, do_sample=False, max_new_tokens=None)[0][\"summary_text\"]\n",
    "            insight = f\"Insight: {summarizer(clean_summary_text[:200], max_length=10, min_length=5, do_sample=False, max_new_tokens=None)[0]['summary_text']}\"\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error summarizing {deck_name}: {e}\")\n",
    "            summary = f\"Summary not available for {deck_name}\"\n",
    "            insight = f\"Insight not available for {deck_name}\"\n",
    "\n",
    "        try:\n",
    "            full_text = \" \".join(text)\n",
    "            classification_input = truncate_to_max_tokens(full_text, max_tokens=512)\n",
    "            candidate_labels = [\"Fintech\", \"HealthTech\", \"SaaS\", \"B2C\", \"Social Media\", \"Food Delivery\", \"Ride-Sharing\"]\n",
    "            industry = zero_shot(classification_input, candidate_labels=candidate_labels, multi_label=False)\n",
    "            industry_label = industry[\"labels\"][industry[\"scores\"].index(max(industry[\"scores\"]))]\n",
    "            # Fallback: Keyword-based classification\n",
    "            if industry_label not in [\"Fintech\", \"HealthTech\", \"SaaS\", \"B2C\", \"Social Media\", \"Food Delivery\", \"Ride-Sharing\"]:\n",
    "                doc = nlp(full_text)\n",
    "                keywords = {\n",
    "                    \"Fintech\": [(\"payment\", 2), (\"finance\", 1.5), (\"banking\", 1.5), (\"transaction\", 1)],\n",
    "                    \"HealthTech\": [(\"health\", 2), (\"medical\", 1.5), (\"patient\", 1), (\"care\", 1)],\n",
    "                    \"SaaS\": [(\"software\", 2), (\"subscription\", 1.5), (\"cloud\", 1), (\"tool\", 1)],\n",
    "                    \"B2C\": [(\"consumer\", 2), (\"marketplace\", 1.5), (\"booking\", 1), (\"sharing\", 1)],\n",
    "                    \"Social Media\": [(\"social\", 2), (\"network\", 1.5), (\"connect\", 1), (\"community\", 1)],\n",
    "                    \"Food Delivery\": [(\"delivery\", 2), (\"restaurant\", 1.5), (\"food\", 1.5), (\"courier\", 1)],\n",
    "                    \"Ride-Sharing\": [(\"ride\", 2), (\"transport\", 1.5), (\"driver\", 1), (\"car\", 1)]\n",
    "                }\n",
    "                keyword_scores = {label: 0 for label in candidate_labels}\n",
    "                for label, kws in keywords.items():\n",
    "                    for kw, weight in kws:\n",
    "                        keyword_scores[label] += weight * full_text.lower().count(kw)\n",
    "                for ent in doc.ents:\n",
    "                    if ent.label_ in [\"ORG\", \"PRODUCT\"]:\n",
    "                        for label, kws in keywords.items():\n",
    "                            if any(kw[0] in ent.text.lower() for kw in kws):\n",
    "                                keyword_scores[label] += 2\n",
    "                max_score = max(keyword_scores.values(), default=0)\n",
    "                if max_score > 0:\n",
    "                    industry_label = max(keyword_scores, key=keyword_scores.get)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Industry classification failed for {deck_name}: {e}\")\n",
    "            industry_label = \"Unknown\"\n",
    "\n",
    "        # Improvement suggestions\n",
    "        suggestions = []\n",
    "        for dim, score in scores.items():\n",
    "            if score < 5:\n",
    "                suggestions.append(f\"Improve {dim}: Ensure clear, specific details and strong metrics.\")\n",
    "\n",
    "        results.append({\n",
    "            \"Deck\": deck_name,\n",
    "            **scores,\n",
    "            \"Final Score\": final_score,\n",
    "            \"Summary\": summary,\n",
    "            \"Insight\": insight,\n",
    "            \"Industry\": industry_label,\n",
    "            \"Suggestions\": \"; \".join(suggestions) if suggestions else \"No major improvements needed\"\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    # Clustering decks\n",
    "    if len(df) > 1:\n",
    "        dimensions = [\"Problem\", \"Market\", \"Traction\", \"Team\", \"Business Model\", \"Vision/Moat\", \"Confidence\"]\n",
    "        X = df[dimensions].fillna(0)\n",
    "        kmeans = KMeans(n_clusters=min(3, len(df)), random_state=42)\n",
    "        df[\"Cluster\"] = kmeans.fit_predict(X)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQc-iDuUXVtY"
   },
   "source": [
    "# Step 4: Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMnTVKRBi6Ja"
   },
   "source": [
    "### Overview\n",
    "The visualization functions (`create_radar_chart`, `create_bar_chart`, `create_heatmap`, `create_word_cloud`) take the DataFrame output from `process_decks` and create:\n",
    "1. **Radar Chart**: Compares dimension scores (Problem, Market, etc.) across decks.\n",
    "2. **Bar Chart**: Displays final scores for each deck.\n",
    "3. **Heatmap**: Shows correlations between dimension scores.\n",
    "4. **Word Cloud**: Visualizes the frequency of industry labels.\n",
    "\n",
    "These visualizations enhance interpretability of the pitch deck analysis, leveraging the final score (normalized 0–100 from `score_deck`) and dimension scores to provide insights for investors.\n",
    "\n",
    "---\n",
    "\n",
    "### Experimental Aspects\n",
    "- **Radar Chart**: Visualizing seven dimensions in a radar chart is experimental, as it assumes equal importance of dimensions. Weighting dimensions (e.g., Market > Team) could improve relevance.\n",
    "- **Heatmap**: Correlation analysis assumes linear relationships, which may not hold for subjective scores. Non-linear methods (e.g., mutual information) could be explored.\n",
    "- **Word Cloud**: Using frequencies for industry labels is simple but may overemphasize common industries. Alternative visualizations (e.g., pie charts) could provide clearer insights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uCyPdu2NWsLa"
   },
   "outputs": [],
   "source": [
    "def create_radar_chart(df, output_folder):\n",
    "    \"\"\"Create a radar chart comparing decks and save to file.\"\"\"\n",
    "    dimensions = [\"Problem\", \"Market\", \"Traction\", \"Team\", \"Business Model\", \"Vision/Moat\", \"Confidence\"]\n",
    "    fig = go.Figure()\n",
    "    for _, row in df.iterrows():\n",
    "        fig.add_trace(go.Scatterpolar(\n",
    "            r=[row[dim] for dim in dimensions],\n",
    "            theta=dimensions,\n",
    "            fill=\"toself\",\n",
    "            name=row[\"Deck\"]\n",
    "        ))\n",
    "    fig.update_layout(\n",
    "        polar=dict(radialaxis=dict(visible=True, range=[0, 10])),\n",
    "        showlegend=True,\n",
    "        title=\"Pitch Deck Comparison (Radar Chart)\"\n",
    "    )\n",
    "    fig.show()\n",
    "    # Save to file\n",
    "    output_path = os.path.join(output_folder, \"radar_chart.png\")\n",
    "    try:\n",
    "        fig.write_image(output_path)\n",
    "        logging.info(f\"Radar chart saved to {output_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to save radar chart: {e}\")\n",
    "    return fig\n",
    "\n",
    "def create_bar_chart(df, output_folder):\n",
    "    \"\"\"Create a bar chart of final scores and save to file.\"\"\"\n",
    "    fig = px.bar(df, x=\"Deck\", y=\"Final Score\", title=\"Final Scores by Deck\", color=\"Deck\")\n",
    "    fig.show()\n",
    "    # Save to file\n",
    "    output_path = os.path.join(output_folder, \"bar_chart.png\")\n",
    "    try:\n",
    "        fig.write_image(output_path)\n",
    "        logging.info(f\"Bar chart saved to {output_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to save bar chart: {e}\")\n",
    "    return fig\n",
    "\n",
    "def create_heatmap(df, output_folder):\n",
    "    \"\"\"Create a correlation heatmap of dimensions and save to file.\"\"\"\n",
    "    dimensions = [\"Problem\", \"Market\", \"Traction\", \"Team\", \"Business Model\", \"Vision/Moat\", \"Confidence\"]\n",
    "    corr = df[dimensions].corr()\n",
    "    fig = px.imshow(corr, text_auto=True, title=\"Dimension Correlation Heatmap\")\n",
    "    fig.show()\n",
    "    # Save to file\n",
    "    output_path = os.path.join(output_folder, \"heatmap.png\")\n",
    "    try:\n",
    "        fig.write_image(output_path)\n",
    "        logging.info(f\"Heatmap saved to {output_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to save heatmap: {e}\")\n",
    "    return fig\n",
    "\n",
    "def create_word_cloud(df, output_folder):\n",
    "    \"\"\"Create a word cloud from industry labels and save to file.\"\"\"\n",
    "    if \"Industry\" not in df.columns or df[\"Industry\"].isna().all():\n",
    "        logging.warning(\"No industry labels available for word cloud\")\n",
    "        return None\n",
    "    # Create frequency dictionary of industry labels\n",
    "    industry_counts = df[\"Industry\"].value_counts().to_dict()\n",
    "    # Generate word cloud with frequencies\n",
    "    wordcloud = WordCloud(\n",
    "        width=800,\n",
    "        height=400,\n",
    "        background_color=\"white\",\n",
    "        min_font_size=10,\n",
    "        max_font_size=100\n",
    "    ).generate_from_frequencies(industry_counts)\n",
    "    # Convert to Plotly figure\n",
    "    fig = go.Figure()\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Word Cloud of Industry Labels\")\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format=\"png\")\n",
    "    buf.seek(0)\n",
    "    img_str = \"data:image/png;base64,\" + base64.b64encode(buf.read()).decode()\n",
    "    # Save word cloud as image\n",
    "    output_path = os.path.join(output_folder, \"word_cloud.png\")\n",
    "    try:\n",
    "        wordcloud.to_file(output_path)\n",
    "        logging.info(f\"Word cloud saved to {output_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to save word cloud: {e}\")\n",
    "    buf.close()\n",
    "    plt.close()\n",
    "    fig.add_layout_image(\n",
    "        dict(\n",
    "            source=img_str,\n",
    "            xref=\"paper\", yref=\"paper\",\n",
    "            x=0, y=1,\n",
    "            sizex=1, sizey=1,\n",
    "            xanchor=\"left\", yanchor=\"top\",\n",
    "            layer=\"below\"\n",
    "        )\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        title=\"Word Cloud of Industry Labels\",\n",
    "        xaxis=dict(visible=False),\n",
    "        yaxis=dict(visible=False),\n",
    "        width=800,\n",
    "        height=400\n",
    "    )\n",
    "    fig.show()\n",
    "    return fig\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KdlpmwxOi6Ja"
   },
   "source": [
    "### Overview\n",
    "----\n",
    "\n",
    "The `create_dashboard` function creates an interactive web-based dashboard using Dash, a Python framework for building analytical applications. The dashboard consolidates:\n",
    "- **Visualizations**: Radar chart, bar chart, heatmap, and word cloud from the respective visualization functions.\n",
    "- **Table**: A detailed table displaying all columns from the `process_decks` DataFrame (e.g., Deck, dimension scores, Final Score, Summary, Insight, Industry, Suggestions, Cluster).\n",
    "- **Purpose**: Provides a user-friendly interface for investors to explore pitch deck analysis results, leveraging the final score (0–100) and dimension scores (0–10) computed by `score_deck` and processed by `process_decks`.\n",
    "\n",
    "#### Purpose\n",
    "Creates an interactive Dash dashboard to display visualizations and a results table, consolidating the analysis from `process_decks` for investor review.\n",
    "- **Dashboard Functionality**:\n",
    "  - Displays a radar chart (dimension scores), bar chart (final scores), heatmap (dimension correlations), word cloud (industry labels), and a table of all DataFrame columns.\n",
    "  - Integrates results from `process_decks`, with final scores (0–100) from `score_deck` shown in the bar chart and table, and dimension scores (0–10) in the radar chart and table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YQL9PVWe0YJC"
   },
   "outputs": [],
   "source": [
    "def create_dashboard(df, output_folder):\n",
    "    \"\"\"Create an interactive Dash dashboard.\"\"\"\n",
    "    app = dash.Dash(__name__)\n",
    "    word_cloud_fig = create_word_cloud(df, output_folder)\n",
    "    app.layout = html.Div([\n",
    "        html.H1(\"Pitch Deck Evaluation Dashboard\"),\n",
    "        html.H2(\"Scores Table\"),\n",
    "        dcc.Graph(figure=create_radar_chart(df, output_folder)),\n",
    "        dcc.Graph(figure=create_bar_chart(df, output_folder)),\n",
    "        dcc.Graph(figure=create_heatmap(df, output_folder)),\n",
    "        html.H2(\"Word Cloud of Industry Labels\"),\n",
    "        dcc.Graph(figure=word_cloud_fig) if word_cloud_fig else html.P(\"No word cloud available\"),\n",
    "        html.H2(\"Detailed Results\"),\n",
    "        html.Table([\n",
    "            html.Thead(html.Tr([html.Th(col) for col in df.columns])),\n",
    "            html.Tbody([\n",
    "                html.Tr([html.Td(df.iloc[i][col]) for col in df.columns])\n",
    "                for i in range(len(df))\n",
    "            ])\n",
    "        ])\n",
    "    ])\n",
    "    return app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9ykWQNfXXNL"
   },
   "source": [
    "# Main Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHCH5mFAi6Ja"
   },
   "source": [
    "### Overview\n",
    "The main execution block is the entry point for running the pitch deck analysis pipeline. It:\n",
    "1. **Processes PDFs**: Calls `process_decks` to extract text, categorize, score, summarize, classify industries, and cluster decks.\n",
    "2. **Saves Results**: Stores the DataFrame to a CSV file.\n",
    "3. **Prints Insights**: Displays the DataFrame, top/bottom 3 decks by final score, and formatted summaries/insights.\n",
    "4. **Launches Dashboard**: Runs the Dash dashboard via `create_dashboard`.\n",
    "5. **Downloads Files**: Downloads the CSV and visualization PNGs in Google Colab.\n",
    "\n",
    "The main block uses the `Final Score` to rank decks and display results.\n",
    "\n",
    "---\n",
    "\n",
    "#### Logic Breakdown\n",
    "1. **Folder Setup**:\n",
    "   - Defines `folder_path = \"/content/pdf_decks\"` (input directory for PDFs) and `output_folder = \"/content/output\"` (output directory for CSVs and PNGs).\n",
    "   - These paths are typical for Google Colab, where `/content` is the default working directory.\n",
    "\n",
    "2. **Process Decks**:\n",
    "   - Calls `process_decks(folder_path, output_folder)` to:\n",
    "     - Extract text from PDFs (e.g., Airbnb, Uber) using `extract_text_hybrid_easyocr`.\n",
    "     - Categorize text into sections (e.g., Problem, Market) with `categorize_text`.\n",
    "     - Score dimensions and compute the final score with `score_deck`.\n",
    "     - Generate summaries and insights with `summarizer` (T5).\n",
    "     - Classify industries with `zero_shot` (BART) and keyword fallback.\n",
    "     - Cluster decks with KMeans.\n",
    "   - Returns a DataFrame (`df`) with columns: Deck, Problem, Market, Traction, Team, Business Model, Vision/Moat, Confidence, Final Score, Summary, Insight, Industry, Suggestions, Cluster.\n",
    "\n",
    "3. **Save Results**:\n",
    "   - Saves the DataFrame to `results.csv` in `output_folder` using `df.to_csv(csv_path, index=False)`.\n",
    "   - Logs the save operation with `logging.info`.\n",
    "\n",
    "4. **Print Results**:\n",
    "   - **Header**: Prints \"=== Pitch Deck Evaluation Dashboard ===\".\n",
    "   - **Full DataFrame**: Displays `df` with all columns and rows.\n",
    "   - **Top 3 Decks**: Uses `df.nlargest(3, \"Final Score\")` to show the top 3 decks by Final Score, displaying Deck, Final Score, and Industry.\n",
    "   - **Bottom 3 Decks**: Uses `df.nsmallest(3, \"Final Score\")` to show the bottom 3 decks.\n",
    "   - **Summaries and Insights**:\n",
    "     - Iterates through `df` rows, printing for each deck:\n",
    "       - Deck name and Industry (e.g., \"uber-pitch-deck (Ride-Sharing)\").\n",
    "       - Final Score (e.g., 70.3).\n",
    "       - Insight (from `process_decks`).\n",
    "       - Summary, formatted with bullet points (replaces \". \" with \".\\n- \" for readability).\n",
    "\n",
    "5. **Launch Dashboard**:\n",
    "   - Calls `create_dashboard(df, output_folder)` to create a Dash app with:\n",
    "     - Radar chart (dimension scores), bar chart (final scores), heatmap (dimension correlations), word cloud (industry labels), and a results table.\n",
    "   - Runs the app with `app.run(debug=True)`, launching a web server (default: `http://127.0.0.1:8050`).\n",
    "\n",
    "6. **Download Files (Colab)**:\n",
    "   - Checks if `files` (from `google.colab.files`) is defined to confirm Colab environment.\n",
    "   - Downloads `results.csv`, `radar_chart.png`, `bar_chart.png`, `heatmap.png`, and `word_cloud.png` using `files.download`.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "- **Execution Flow**:\n",
    "  - Processes PDFs with `process_decks`, saves results to `results.csv`, prints the DataFrame, top/bottom 3 decks by Final Score, and formatted summaries/insights, launches the Dash dashboard, and downloads outputs in Colab.\n",
    "- **Final Score Integration**:\n",
    "  - Used to rank decks (`nlargest`, `nsmallest`), displayed in the bar chart and table via `create_dashboard`.\n",
    "  - Example: Uber (Final Score: 70.3) appears in the bar chart and top/bottom rankings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "E9SyDBpOWwTZ",
    "outputId": "221cb731-c8ab-437e-87f1-5acfc338f277"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    folder_path = \"/content/pdf_decks\"\n",
    "    output_folder = \"/content/output\"\n",
    "    df = process_decks(folder_path, output_folder)\n",
    "    # Save DataFrame to CSV\n",
    "    csv_path = os.path.join(output_folder, \"results.csv\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    logging.info(f\"DataFrame saved to {csv_path}\")\n",
    "    print(\"=== Pitch Deck Evaluation Dashboard ===\")\n",
    "    print(df)\n",
    "    print(\"\\nTop 3 Decks:\")\n",
    "    print(df.nlargest(3, \"Final Score\")[[\"Deck\", \"Final Score\", \"Industry\"]])\n",
    "    print(\"\\nBottom 3 Decks:\")\n",
    "    print(df.nsmallest(3, \"Final Score\")[[\"Deck\", \"Final Score\", \"Industry\"]])\n",
    "    print(\"\\n=== Summaries and Insights ===\")\n",
    "    for _, row in df.iterrows():\n",
    "        print(f\"Deck: {row['Deck']} ({row['Industry']})\")\n",
    "        print(f\"Final Score: {row['Final Score']}\")\n",
    "        print(f\"Insight: {row['Insight']}\")\n",
    "        summary_formatted = row[\"Summary\"].replace(\". \", \".\\n- \")\n",
    "        print(f\"Summary:\\n- {summary_formatted}\")\n",
    "        print()\n",
    "    app = create_dashboard(df,output_folder)\n",
    "    app.run(debug=True)\n",
    "    if files is not None:  # Running in Colab\n",
    "        files.download(csv_path)\n",
    "        files.download(os.path.join(output_folder, \"radar_chart.png\"))\n",
    "        files.download(os.path.join(output_folder, \"bar_chart.png\"))\n",
    "        files.download(os.path.join(output_folder, \"heatmap.png\"))\n",
    "        files.download(os.path.join(output_folder, \"word_cloud.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nxGazUHAi6Jc"
   },
   "source": [
    "Thank you for the opportunity to discuss my pitch deck analysis pipeline. Given the time constraints, I developed a robust system that extracts text from PDFs, categorizes it into investor-relevant sections, scores dimensions like Problem and Market using BART and VADER, and computes a normalized final score (0–100) based on a weighted sum of seven dimensions. It also generates summaries, industry classifications, and interactive visualizations via a Dash dashboard, providing a comprehensive tool for investors as of July 24, 2025.\n",
    "\n",
    "With more time, I would focus on three key improvements to enhance accuracy and usability:\n",
    "\n",
    "1. **Dynamic File Processing**: Replace hardcoded deck names with dynamic file discovery using `os.listdir` to scale the pipeline for any number of PDFs, improving flexibility for real-world applications.\n",
    "\n",
    "2. **Advanced NLP Models**: Fine-tune a domain-specific model like Llama 3 for scoring and industry classification, replacing the general-purpose BART model to boost accuracy for pitch deck-specific language.\n",
    "\n",
    "3. **Interactive Dashboard**: Add filters to the Dash dashboard, such as deck or industry selectors, to allow investors to focus on specific results, enhancing usability and engagement.\n",
    "\n",
    "These enhancements would make the pipeline more scalable, precise, and investor-friendly, aligning with the needs of modern startup evaluation.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
